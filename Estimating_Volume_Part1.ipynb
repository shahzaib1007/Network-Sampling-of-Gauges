{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import os\n",
    "import import_ipynb\n",
    "import glob\n",
    "import warnings\n",
    "import ee\n",
    "import geopandas as gpd\n",
    "ee.Initialize()\n",
    "import geemap\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# from Master_phd import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pd.read_csv(r'Path the file having all the water surface elevation data from each water body')\n",
    "g.head(3)\n",
    "\"\"\" The columns of the data file here are [Unnamed: 0 gauge_id\tGauge\tdate\theight\tLatitude\tLongitude\tNumber of Readings]\n",
    "Where \"gauge_id\" and \"Unnamed: 0\" is the id associated with each gauge. \"Gauge\" is the name of the waterbody where gauge is installed. \n",
    "\"date\" is the date on which the observation was collected. \"height\" is water surface elevation. \n",
    "\"Latitude\" and \"Longitude\" are the coordinates of the gauge.\n",
    "\"Number of Readings\" are the total number of readings from the gauge since it was installed. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Storing all the names of the water body where we have the water surface elevation data \"\"\"\n",
    "names = list(set(g['Gauge']))\n",
    "haor_names = []\n",
    "for name in names:\n",
    "    haor_names.append(name.replace(' ','_'))\n",
    "haor_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Removing the unnecessary columns. These columns are extra information which is not required for analysis \"\"\"\n",
    "for i in range(len(names)):\n",
    "    filtered_g = g[g['Gauge'] == names[i].replace('_',' ')]\n",
    "    filtered_g = filtered_g.drop(columns =['Latitude','Longitude','Unnamed: 0']).reset_index(drop = True)\n",
    "    filtered_g.to_csv(r'Path to store the elevation files'+filtered_g['gauge_id'][0]+\"_\"+names[i].replace(\" \",\"_\")+'_elevation.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining all the required functions. These functions are from my master_phd notebook, but I am directly pasting the function for user needs\"\"\"\n",
    "\n",
    "def MAKE_LOCSS_ELEVATION(file_height,path,UID,haorname,unit='meter'):\n",
    "    f = file_height\n",
    "    f1 = f.reset_index(drop = True)\n",
    "    # columns_to_retain = ['date','height']\n",
    "#     f1 = f1.drop(columns=[col for col in df if col not in columns_to_retain])\n",
    "#     f1 = f1.drop(columns=['Notes'])\n",
    "    f1 = f1.rename(columns = {'date':'Date','height':'Height(ft)'})\n",
    "    f1 = f1[['Date','Height(ft)','Gauge','gauge_id']]\n",
    "    \n",
    "    m = np.quantile(f1['Height(ft)'],0.95)\n",
    "    Quantile90_threshold_noise_list=[]\n",
    "    for k in range(len(f1)):\n",
    "        if f1['Height(ft)'][k]>=m:\n",
    "            Quantile90_threshold_noise_list.append(k)\n",
    "    f1 = f1.drop(f1.index[Quantile90_threshold_noise_list])\n",
    "    f1 = f1.reset_index(drop = True)\n",
    "    \n",
    "    if unit =='feet':\n",
    "        f1['Height(ft)'] = f1['Height(ft)']*0.3048\n",
    "    elif unit =='meter':\n",
    "        f1['Height(ft)'] = f1['Height(ft)']\n",
    "    else:\n",
    "        print('Wrong unit passed')\n",
    "        exit()\n",
    "    f1 = f1.rename(columns={'Height(ft)':'Height in meters'})\n",
    "    f1.to_csv(path+'\\\\Corrected_'+UID+\"_\"+haorname+'.csv',index = False)\n",
    "    print('finished',haorname)\n",
    "    return f1.head()\n",
    "\n",
    "def SORTING_AREA_DATA_DATEUPDATED(waterareafile,elevationfile,path ,haorname,technique):\n",
    "    f = waterareafile\n",
    "    g = elevationfile\n",
    "    f['Date'] = pd.to_datetime(f['Date'], format='%Y-%m-%d')\n",
    "    for area_range in range(len(f)):\n",
    "        f['Date'][area_range] = f['Date'][area_range].date()\n",
    "    for elev_range in range(len(g)):\n",
    "        date_time_str = g['Date'][elev_range] \n",
    "        g['Date'][elev_range] = datetime.datetime.strptime(date_time_str, '%m/%d/%Y')\n",
    "        g['Date'][elev_range] = g['Date'][elev_range].date()\n",
    "    g1 = pd.DataFrame(np.zeros([len(g), 2])*np.nan,columns = ['Date','Area'])\n",
    "    g1['Date'] = g['Date']\n",
    "    f1 = pd.concat([f, g1],axis = 0, ignore_index=True)\n",
    "    f1 = f1.sort_values(by=['Date'])     \n",
    "    f1.to_csv(path+haorname+\"\\\\\"+'GEE_'+technique+'_'+haorname+\"_dateupdated.csv\",index=False)\n",
    "    return f1.head()\n",
    "\n",
    "def STORAGE_CHANGE(areafile,elevfile,path,UID,haorname,technique,save_pic=True,save_csv=True):\n",
    "    f = areafile\n",
    "    g = elevfile\n",
    "    f['Date'] = pd.to_datetime(f['Date'], format='%Y-%m-%d')\n",
    "    kw = dict(limit_direction='forward')\n",
    "    f1 = f.copy()\n",
    "    f1['Area'] = f1['Area'].interpolate()\n",
    "    f1['Date_status'] = f1['Date'].isin(g['Date'])\n",
    "    f7 = f1[f1['Date_status'] ==True ]\n",
    "    f7 =f7.drop_duplicates(subset = ['Date'])\n",
    "    f7 = f7.drop(columns = ['Date_status'])\n",
    "    g2 = g.drop_duplicates(subset=['Date'])\n",
    "    f7 = f7.reset_index().drop(columns=['index'])\n",
    "    g2 = g2.reset_index().drop(columns=['index'])\n",
    "    Est_Vol = [np.nan]\n",
    "    for i in range(1,len(f7)):\n",
    "        Est_Vol.append((f7['Area'][i]+f7['Area'][i-1])*(0.5)*((g2['Height in meters'][i]-g2['Height in meters'][i-1])))\n",
    "    f7.insert(2,'Height(m)',g2[\"Height in meters\"])\n",
    "    f7.insert(3,'Est_Vol_Change(km3)',Est_Vol)\n",
    "    fig,ax = plt.subplots(figsize = (14,6))\n",
    "    ax.plot(f7['Date'],f7['Est_Vol_Change(km3)'],label = 'Storage Change')\n",
    "    ax.set_xlabel('Date',fontsize = 14)\n",
    "    ax.set_ylabel('Storage Change',fontsize = 14)\n",
    "    ax.set_title('Storage Change of '+ haorname+ 'Haor '+ technique,fontsize = 16)\n",
    "    fig.autofmt_xdate()\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(path+\"\\\\\"+haorname+\"\\\\\",exist_ok=True)\n",
    "    if save_csv ==True:    \n",
    "        f7.to_csv(path+\"\\\\\"+haorname+\"\\\\\"+UID+'_'+haorname+'_'+technique+'.csv')\n",
    "    path_pic =  path.split('Vol_Est')[0]+'graphs'\n",
    "    os.makedirs(path_pic+\"\\\\\"+haorname+\"\\\\\",exist_ok=True)\n",
    "    if save_pic ==True:    \n",
    "        plt.savefig(path_pic+\"\\\\\"+haorname+\"\\\\\"+UID+'_'+haorname+\"_Storage_Change_\"+technique) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the elevation data to usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Quantile filtering the data and converting it in meters if it is in feet \"\"\"\n",
    "for i in range(len(names)):\n",
    "    csv_files = glob.glob(r'Path to the elevation files\\\\*'+\"_\"+names[i].replace(\" \",\"_\")+'_elevation.csv')\n",
    "    f = pd.read_csv(csv_files[0])\n",
    "    f = f.reset_index(drop = True)\n",
    "    MAKE_LOCSS_ELEVATION(f,path =r'Path to the elevation files\\Corrected\\\\',UID = f['gauge_id'][0],haorname = names[i].replace(\" \",\"_\"),unit='meter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting the Area Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list=['Alta_Dighi',\n",
    " 'Arali_Bil',\n",
    " 'Asmatpur_Beel',\n",
    " 'Balait_Haor',\n",
    " 'Bayhe_Beel',\n",
    " 'Bhatipara_Haor',\n",
    " 'Bongo_Sonahat',\n",
    " 'Boro_Haor',\n",
    " 'Bukvora_Baor',\n",
    " 'Caran_Beel',\n",
    " 'Chatal_Beel',\n",
    " 'Ciklir_Beel',\n",
    " 'Court_Dighi',\n",
    " 'Cuagachi_Doa',\n",
    " 'Damus',\n",
    " 'Dhala_China_Beel',\n",
    " 'Golar_Beel',\n",
    " 'Hatir_Jheel',\n",
    " 'Jaliyar_Haor',\n",
    " 'Joal_Bhanga_Haor',\n",
    " 'Joysagor',\n",
    " 'kalai_Beel',\n",
    " 'Khoasagar_Dighi',\n",
    " 'Khordo_Baor',\n",
    " 'Kuralgachi_Beel',\n",
    " 'Lusni_Beel_Haor',\n",
    " 'Markas_Beel',\n",
    " 'Mohishuara_Haor',\n",
    " 'Mora_Ganga',\n",
    " 'Mosha_Gatir_Haor',\n",
    " 'Pabiadher_Haor',\n",
    " 'Patharchuli_Haor',\n",
    " 'Ramrai_Dighi',\n",
    " 'Rarikhal',\n",
    " 'Shiruali_Baor',\n",
    " 'Srihar_Beel',\n",
    " 'Subolpur_Beel',\n",
    " 'Sub_Beel_Haor',\n",
    " 'Sunair_Haor', \n",
    " 'Atranga_Dighi',\n",
    " 'Aura_Bora_Beel',\n",
    " 'Baisha_Beel',\n",
    " 'Balashur',\n",
    " 'Bamui_Beel',\n",
    " 'Bijoy_Singha_Dighi',\n",
    " 'Boluhar_Baor',\n",
    " 'Buripota_Beel',\n",
    " 'Calan_Beel',\n",
    " 'Dekhar',\n",
    " 'Dharmo_Sagar',\n",
    " 'Ghora_dighi',\n",
    " 'Hakaluki',\n",
    " 'Kaptai_lake',\n",
    " 'Kataiya_Band',\n",
    " 'Korchar',\n",
    " 'Mohamaya_Lake',\n",
    " 'Moharaja_Dighi',\n",
    " 'Nilsagor',\n",
    " 'Ramsagor',\n",
    " 'Sukan_Dighi',\n",
    " 'Udgal_Beel',\n",
    " 'Vullar_Haor']\n",
    "len(names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sorting the water surface and water surface elevation data as per time and inputting the common dates between the two datasets.\"\"\"\n",
    "for i in range(len(names_list)):\n",
    "    print('Working on ', names_list[i])\n",
    "    data_files = glob.glob(r'Path to Water Surface Area files'+names_list[i]+\"\\\\\"+'GEE_*'+'.csv')\n",
    "    for k in range(len(data_files)):\n",
    "        area_files = data_files[k]\n",
    "        area_file = pd.read_csv(area_files)\n",
    "        elev_files = glob.glob(r'Path to the elevation files\\Corrected\\\\'+'Corrected_*'+names_list[i]+'.csv')\n",
    "        elev_file = pd.read_csv(elev_files[0])\n",
    "        try:\n",
    "            SORTING_AREA_DATA_DATEUPDATED(area_file,elev_file,path = r'Path to Water Surface Area files' ,haorname = names_list[i],technique=area_files.split(\"\\\\GEE_\")[1].split(names_list[0])[0])#area_files.split(\"\\\\GEE_\")[1].split(names_list[0])[0])\n",
    "        except:\n",
    "            print('Did not finish with', names_list[i],'technique',area_files.split(\"\\\\GEE_\")[1].split(names_list[0])[0])\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the names of the wetlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(r\"Path to store wetlands name\\Wetlands_Names.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(names_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the Volume Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"Path to store wetlands name\\Wetlands_Names.txt\", \"rb\") as fp:   # Unpickling\n",
    "    names_list= pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:\\Users\\skhan7\\Desktop\\Research\\PhD\\Chapter1\\Vol_Est\\\\'\n",
    "path.split('Vol_Est')[0]+'graphs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Interpolating the data where there is no data between common dates and estimating the volume change using trapezoidal method. \"\"\"\n",
    "for i in range(len(names_list)):\n",
    "    print('Working on ', names_list[i])\n",
    "    data_files = glob.glob(r'Path to Water Surface Area files'+names_list[i]+\"\\\\\"+'GEE_*'+'dateupdated.csv')\n",
    "    for k in range(len(data_files)):\n",
    "        area_files = data_files[k]\n",
    "        area_file = pd.read_csv(area_files)\n",
    "        elev_files = glob.glob(r'Path to the elevation files\\Corrected\\\\'+'Corrected_*'+names_list[i]+'.csv')\n",
    "        elev_file = pd.read_csv(elev_files[0])\n",
    "        UID = elev_file['gauge_id'][0]\n",
    "        haorname = names_list[i]\n",
    "        technique=area_files.split('GEE_')[1].split(names_list[i])[0]\n",
    "        STORAGE_CHANGE(area_file,elev_file,path=r'Path to store volume change files\\\\',UID=UID,haorname=haorname,technique=technique,save_pic=True,save_csv=True)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(area_file['Area']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(area_file['Date'],area_file['Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_file['Area'] = area_file['Area'].interpolate()\n",
    "area_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(area_file['Date'],area_file['Area'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
